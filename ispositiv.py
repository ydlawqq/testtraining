# -*- coding: utf-8 -*-
"""IsPositiv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1idW80CRh-_M2T5LJs37nZejV2q0ovqhZ
"""

from datasets import load_dataset, DatasetDict

import unicodedata

import html

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding

ds = load_dataset("SetFit/bbc-news")

ds.reset_format()

ds['train'].shuffle(seed=42)['text'][0]

def space_delitter(x):
    return {'text': [" ".join(html.unescape(i).split()).lower() for i in x['text']]}

ds = ds.map(space_delitter, batched=True)

ds['train']['text'][0]

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

import string

simbols = list(string.ascii_lowercase) + ['!', ',', '"', '.', '?', '-', '+', '=', '(', ')', '&', ' ']

itg = []
for i in ''.join(ds['train']['text']):
    if i not in simbols+itg:
        itg.append(i)

itg

ds = ds.map(lambda x: {'text': unicodedata.normalize("NFC", x['text'])})

ds['train']['text'][0]

unicodedata.normalize("NFKD", 'Ã©')

def tokenize_func(example):
    return {
        'tokenized_text':
        tokenizer(example['text'], truncation=True, max_length=512, padding=True)
    }

ds = ds.map(tokenize_func)

tokens = ds['train']['tokenized_text'][0]
tokenizer.decode(tokens['input_ids'])

model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)

try:
    ds['train']['label'].features.names
except AttributeError:
    print('Dataset labels has no encoded names')

ds.set_format('pandas')

labels = ds['train']['label'].unique()

labels_encode = ds['train']['label_text'].unique()
labels_encode

label_to_category = dict(zip(labels, labels_encode))

label_to_category

ds.reset_format()

def extract_tokens(example):
    example['input_ids'] = example['tokenized_text']['input_ids']
    example['attention_mask'] = example['tokenized_text']['attention_mask']
    return example

ds = ds.map(extract_tokens, remove_columns=['text','label_text', 'tokenized_text'])

ds

args = TrainingArguments(
    output_dir ='./model',
    eval_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=1,
    weight_decay=0.01,
    report_to=[]
)

from sklearn.metrics import accuracy_score
import numpy as np

def compute_metrics(pred):
    logits, labels = pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, preds)



    }

data_collator = DataCollatorWithPadding(
    tokenizer=tokenizer,
    padding='max_length',
    max_length=512
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=ds['train'],
    eval_dataset=ds['train'],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    data_collator=data_collator

)

trainer.train()

trainer.predict(ds['test'])

